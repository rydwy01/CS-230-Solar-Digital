{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uxeU8ktqNA0d",
    "outputId": "f07cc5e1-2489-41b1-cc12-4a0a27bc9551"
   },
   "source": [
    "Cloning the github repo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo\n",
    "# !git clone https://github.com/qubvel/segmentation_models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "__DGphOwgyLe",
    "outputId": "75677f1f-1168-4a2d-be19-bd55deb88292"
   },
   "outputs": [],
   "source": [
    "%cd segmentation_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UBD0A1YqhF6u",
    "outputId": "17f712e3-8ad8-479f-a13f-6a443d486b27"
   },
   "outputs": [],
   "source": [
    "# Visualise dependencies\n",
    "# !cat requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the dataset!!\n",
    "\n",
    "def load_data(folder_name, img_size):\n",
    "    \n",
    "    # Extract the names of images!!\n",
    "    data = pd.read_csv(folder_name + '.csv', header = None)\n",
    "    names = data[0].to_numpy()\n",
    "    \n",
    "    # Number of images (image + mask)\n",
    "    n = int(len(names) / 2)\n",
    "    # List for dataset (X,Y)\n",
    "    data = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Read the i'th image\n",
    "        im = Image.open(folder_name + '/' + names[2 * i])\n",
    "        im = im.resize((img_size, img_size))\n",
    "        tpx = np.array(im)\n",
    "        # Read the i'th mask layer\n",
    "        im = Image.open(folder_name + '/' + names[2 * i + 1])\n",
    "        im = im.resize((img_size, img_size))\n",
    "        tpy = np.array(im)\n",
    "        # Binary Mask layer!!\n",
    "        tpy[tpy > 1] = 1\n",
    "        tpy = tpy.astype(float)\n",
    "\n",
    "        data.append((tpx,tpy))\n",
    "    # For loop ends here!!\n",
    "    \n",
    "    return(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "u9Rll87g61hW"
   },
   "outputs": [],
   "source": [
    "# Dataset directory!\n",
    "path = '/home/ubuntu/Dataset/Segmentation/'\n",
    "\n",
    "# Folder names for each dataset!\n",
    "pv1 = ['PV01_Brick', 'PV01_Concrete', 'PV01_SteelTile']\n",
    "pv3 = ['PV03_Cropland', 'PV03_Grassland', 'PV03_Rooftop', 'PV03_SalineAlkali', 'PV03_Shrubwood', 'PV03_WaterSurface']\n",
    "pv8 = ['PV08_Ground', 'PV08_Rooftop']\n",
    "\n",
    "# Data for rooftop training!!\n",
    "# pv3 = ['PV03_Rooftop']\n",
    "# pv8 = ['PV08_Rooftop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3cHVEKohS6f",
    "outputId": "b77d5595-440e-43cc-949b-907c86eb9df2"
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "img_size = 224 # size of input image\n",
    "\n",
    "# First dataset\n",
    "dataset = load_data(path + pv1[0], img_size)\n",
    "\n",
    "# All other datasets\n",
    "pv_others = pv1[1:] + pv3 + pv8\n",
    "\n",
    "for pv in pv_others:\n",
    "    temp = load_data(path + pv, img_size)\n",
    "    dataset = dataset + temp\n",
    "\n",
    "# Shuffling the samples\n",
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of samples: \" + str(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "mhjPz_YS5HJo",
    "outputId": "f35fad95-53c4-42bc-b406-3337247d2935"
   },
   "outputs": [],
   "source": [
    "# Plot some samples!\n",
    "np.random.seed(1)\n",
    "idx = np.random.randint(low = 0, high = len(dataset), size = 5)\n",
    "\n",
    "for i in idx:\n",
    "    img, mask = dataset[i]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title('Image ' + str(i))\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(mask)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title('Mask ' + str(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing segmentation models library\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "\n",
    "import segmentation_models as sm\n",
    "\n",
    "sm.set_framework('tf.keras')\n",
    "\n",
    "keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader function to load each training batch lazily!\n",
    "class Dataloder(keras.utils.Sequence):\n",
    "    \"\"\"Load data from dataset and form batches\n",
    "    \n",
    "    Args:\n",
    "        dataset: instance of Dataset class for image loading and preprocessing.\n",
    "        batch_size: Integet number of images in batch.\n",
    "        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(dataset))\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # collect batch data\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "        \n",
    "        # transpose list of lists\n",
    "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return len(self.indexes) // self.batch_size\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the backbone\n",
    "BACKBONE = 'resnet34'\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.0001 # learning rate\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset 90%:5%:5% train:dev:test split\n",
    "n = len(dataset)\n",
    "m = int(n * 0.9)\n",
    "v = m + int((n-m) / 2)\n",
    "\n",
    "train = dataset[0:m]\n",
    "val = dataset[m:v]\n",
    "test = dataset[v:]\n",
    "\n",
    "# Data loader for training\n",
    "train_dataloader = Dataloder(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = Dataloder(val, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vz077OMlFnqU"
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = sm.Unet(BACKBONE, encoder_weights='imagenet', classes=1, activation='sigmoid', encoder_freeze = True)\n",
    "\n",
    "# Define optomizer\n",
    "optim = keras.optimizers.Adam(LR)\n",
    "\n",
    "# Define the loss (for our case it's binary!)\n",
    "loss = sm.losses.bce_jaccard_loss\n",
    "\n",
    "# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n",
    "# jaccard_loss = sm.losses.JaccardLoss()\n",
    "# bce_loss = sm.losses.BinaryCELoss()\n",
    "# loss = jaccard_loss + (1 * bce_loss) # To try different combination of loss functions!!\n",
    "\n",
    "# Define the metrics\n",
    "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n",
    "\n",
    "# Compile keras model with defined optimozer, loss and metrics\n",
    "model.compile(optim, loss, metrics)\n",
    "\n",
    "# define callbacks for learning rate scheduling and best checkpoints saving\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('./best_model.h5', save_weights_only=True, save_best_only=True, mode='min'),\n",
    "    keras.callbacks.ReduceLROnPlateau(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning to keep the properly trained encoder weights (Note: set encoder_freeze = True)\n",
    "\n",
    "# pretrain model decoder\n",
    "model.fit(\n",
    "    train_dataloader, \n",
    "    steps_per_epoch=len(train_dataloader), \n",
    "    epochs=2, \n",
    "    validation_data=valid_dataloader, \n",
    "    validation_steps=len(valid_dataloader),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all the layers trainable!\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Re-compile the model\n",
    "optim = keras.optimizers.Adam(LR/10) # Reduce the learning rate if needed\n",
    "model.compile(optim, loss, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bL2WeutfpS3Z",
    "outputId": "22812400-cced-4181-b70f-fe5ecee665e2"
   },
   "outputs": [],
   "source": [
    "# Continue training\n",
    "history = model.fit_generator(\n",
    "    train_dataloader, \n",
    "    steps_per_epoch=len(train_dataloader), \n",
    "    epochs=EPOCHS, \n",
    "    callbacks=callbacks, \n",
    "    validation_data=valid_dataloader, \n",
    "    validation_steps=len(valid_dataloader),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "-Ogs59DapS9N",
    "outputId": "a82b2ae3-a613-4d3e-9476-36ed136fd34c"
   },
   "outputs": [],
   "source": [
    "# Plot training & validation iou_score values\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Loss', 'Validation Loss'], loc='upper left')\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['iou_score'])\n",
    "plt.plot(history.history['val_iou_score'])\n",
    "plt.title('Model iou_score')\n",
    "plt.ylabel('iou_score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training IoU', 'Validation IoU'], loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "# plt.savefig('loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = Dataloder(test, batch_size=1, shuffle=False)\n",
    "\n",
    "# load best weights\n",
    "model.load_weights('best_model.h5') \n",
    "\n",
    "scores = model.evaluate_generator(test_dataloader)\n",
    "\n",
    "print(\"Loss: {:.4}\".format(scores[0]))\n",
    "for metric, value in zip(metrics, scores[1:]):\n",
    "    print(\"mean {}: {:.4}\".format(metric.__name__, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 974
    },
    "id": "VxrhworDC9l-",
    "outputId": "4cf27fdf-0c89-4b6e-c1ea-a8e660ded3a5"
   },
   "outputs": [],
   "source": [
    "# Predictions on Test dataset!\n",
    "samples = np.random.randint(low = 0, high = len(test), size = 5)\n",
    "\n",
    "for j in samples:\n",
    "    x, y = test[j]\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(x)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title('Image')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.imshow(y)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title('True Mask')\n",
    "    \n",
    "    mask = model.predict(x.reshape(1,img_size,img_size,3)).round()\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(mask.reshape(img_size,img_size))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title('Predicted Mask')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JIhJbBfpTIS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_ogC1-QnlZf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Segmentation_Trial1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
